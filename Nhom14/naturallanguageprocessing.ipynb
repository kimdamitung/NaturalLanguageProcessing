{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk, pydash, math, os, itertools\n",
    "from pydash import flatten, flatten_deep\n",
    "from collections import Counter, OrderedDict\n",
    "from frozendict import frozendict\n",
    "from humanize import intcomma\n",
    "from operator import itemgetter\n",
    "from typing import *\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import product, combinations\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10869</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10870</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows ร 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword location                                               text  \\\n",
       "id                                                                          \n",
       "1         NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "4         NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "5         NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "6         NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "7         NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "...       ...      ...                                                ...   \n",
       "10869     NaN      NaN  Two giant cranes holding a bridge collapse int...   \n",
       "10870     NaN      NaN  @aria_ahrary @TheTawniest The out of control w...   \n",
       "10871     NaN      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "10872     NaN      NaN  Police investigating after an e-bike collided ...   \n",
       "10873     NaN      NaN  The Latest: More Homes Razed by Northern Calif...   \n",
       "\n",
       "       target  \n",
       "id             \n",
       "1           1  \n",
       "4           1  \n",
       "5           1  \n",
       "6           1  \n",
       "7           1  \n",
       "...       ...  \n",
       "10869       1  \n",
       "10870       1  \n",
       "10871       1  \n",
       "10872       1  \n",
       "10873       1  \n",
       "\n",
       "[7613 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('dataset/train.csv', index_col=0)\n",
    "df_test  = pd.read_csv('dataset/test.csv', index_col=0)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "[nan, 'ablaze', 'accident', 'aftershock', 'airplane%20accident', 'ambulance', 'annihilated', 'annihilation', 'apocalypse', 'armageddon', 'army', 'arson', 'arsonist', 'attack', 'attacked', 'avalanche', 'battle', 'bioterror', 'bioterrorism', 'blaze', 'blazing', 'bleeding', 'blew%20up', 'blight', 'blizzard', 'blood', 'bloody', 'blown%20up', 'body%20bag', 'body%20bagging', 'body%20bags', 'bomb', 'bombed', 'bombing', 'bridge%20collapse', 'buildings%20burning', 'buildings%20on%20fire', 'burned', 'burning', 'burning%20buildings', 'bush%20fires', 'casualties', 'casualty', 'catastrophe', 'catastrophic', 'chemical%20emergency', 'cliff%20fall', 'collapse', 'collapsed', 'collide', 'collided', 'collision', 'crash', 'crashed', 'crush', 'crushed', 'curfew', 'cyclone', 'damage', 'danger', 'dead', 'death', 'deaths', 'debris', 'deluge', 'deluged', 'demolish', 'demolished', 'demolition', 'derail', 'derailed', 'derailment', 'desolate', 'desolation', 'destroy', 'destroyed', 'destruction', 'detonate', 'detonation', 'devastated', 'devastation', 'disaster', 'displaced', 'drought', 'drown', 'drowned', 'drowning', 'dust%20storm', 'earthquake', 'electrocute', 'electrocuted', 'emergency', 'emergency%20plan', 'emergency%20services', 'engulfed', 'epicentre', 'evacuate', 'evacuated', 'evacuation', 'explode', 'exploded', 'explosion', 'eyewitness', 'famine', 'fatal', 'fatalities', 'fatality', 'fear', 'fire', 'fire%20truck', 'first%20responders', 'flames', 'flattened', 'flood', 'flooding', 'floods', 'forest%20fire', 'forest%20fires', 'hail', 'hailstorm', 'harm', 'hazard', 'hazardous', 'heat%20wave', 'hellfire', 'hijack', 'hijacker', 'hijacking', 'hostage', 'hostages', 'hurricane', 'injured', 'injuries', 'injury', 'inundated', 'inundation', 'landslide', 'lava', 'lightning', 'loud%20bang', 'mass%20murder', 'mass%20murderer', 'massacre', 'mayhem', 'meltdown', 'military', 'mudslide', 'natural%20disaster', 'nuclear%20disaster', 'nuclear%20reactor', 'obliterate', 'obliterated', 'obliteration', 'oil%20spill', 'outbreak', 'pandemonium', 'panic', 'panicking', 'police', 'quarantine', 'quarantined', 'radiation%20emergency', 'rainstorm', 'razed', 'refugees', 'rescue', 'rescued', 'rescuers', 'riot', 'rioting', 'rubble', 'ruin', 'sandstorm', 'screamed', 'screaming', 'screams', 'seismic', 'sinkhole', 'sinking', 'siren', 'sirens', 'smoke', 'snowstorm', 'storm', 'stretcher', 'structural%20failure', 'suicide%20bomb', 'suicide%20bomber', 'suicide%20bombing', 'sunk', 'survive', 'survived', 'survivors', 'terrorism', 'terrorist', 'threat', 'thunder', 'thunderstorm', 'tornado', 'tragedy', 'trapped', 'trauma', 'traumatised', 'trouble', 'tsunami', 'twister', 'typhoon', 'upheaval', 'violent%20storm', 'volcano', 'war%20zone', 'weapon', 'weapons', 'whirlwind', 'wild%20fires', 'wildfire', 'windstorm', 'wounded', 'wounds', 'wreck', 'wreckage', 'wrecked']\n"
     ]
    }
   ],
   "source": [
    "keywords = df_train['keyword'].unique().tolist()\n",
    "print(len(keywords))\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(\n",
    "    dfs: List[pd.DataFrame], \n",
    "    keys          = ('text', 'keyword', 'location'), \n",
    "    stemmer       = True, \n",
    "    preserve_case = True, \n",
    "    reduce_len    = False, \n",
    "    strip_handles = True,\n",
    "    use_stopwords = True,\n",
    "    **kwargs,\n",
    ") -> List[List[str]]:\n",
    "    tokenizer = nltk.TweetTokenizer(preserve_case=preserve_case, reduce_len=reduce_len, strip_handles=strip_handles) \n",
    "    porter    = nltk.PorterStemmer()\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english') + [ 'nan' ])\n",
    "    output    = []\n",
    "    for df in flatten([ dfs ]):\n",
    "        for index, row in df.iterrows():\n",
    "            tokens = flatten([ tokenizer.tokenize(str(row[key] or \"\")) for key in keys])\n",
    "            if use_stopwords:\n",
    "                tokens = [ token for token in tokens if token.lower() not in stopwords and len(token) >= 2]                \n",
    "            if stemmer:\n",
    "                tokens = [ porter.stem(token) for token in tokens ]\n",
    "            output.append(tokens)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deed', 'reason', '#earthquak', 'may', 'allah', 'forgiv', 'us'],\n",
       " ['forest', 'fire', 'near', 'la', 'rong', 'sask', 'canada']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_df(df_train)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequencies(df, **kwargs) -> Dict[int, Counter]:\n",
    "    tokens = {\n",
    "        0: flatten(tokenize_df( df[df['target'] == 0], **kwargs )),\n",
    "        1: flatten(tokenize_df( df[df['target'] == 1], **kwargs )),\n",
    "    }\n",
    "    freqs = { \n",
    "        target: Counter(dict(Counter(tokens[target]).most_common())) \n",
    "        for target in [0, 1]\n",
    "    }\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freqs[0] 12811 [('...', 421), ('new', 320), ('like', 309), ('get', 224), ('bodi', 216), (\"i'm\", 207), ('scream', 194), ('รป_', 171), ('burn', 159), ('obliter', 157)]\n",
      "freqs[1] 10795 [('...', 637), ('fire', 303), ('bomb', 242), ('new', 207), ('suicid', 204), ('evacu', 185), ('flood', 176), ('รป_', 171), ('derail', 170), ('kill', 160)]\n"
     ]
    }
   ],
   "source": [
    "freqs = word_frequencies(df_train)\n",
    "print('freqs[0]', len(freqs[0]), freqs[0].most_common(10))\n",
    "print('freqs[1]', len(freqs[1]), freqs[1].most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency( tokens: List[str] ) -> Counter:\n",
    "    tokens = flatten_deep(tokens)\n",
    "    idf = {\n",
    "        token: math.log( len(tokens) / count ) \n",
    "        for token, count in Counter(tokens).items()\n",
    "    }\n",
    "    idf = Counter(dict(Counter(idf).most_common()))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency_df( dfs ) -> Counter:\n",
    "    tokens = flatten_deep([ tokenize_df(df) for df in flatten([ dfs ]) ])\n",
    "    return inverse_document_frequency(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('...', 4.467633783633229),\n",
       " ('new', 5.142574999696602),\n",
       " ('fire', 5.360577151510393),\n",
       " ('like', 5.413220884995814),\n",
       " ('รป_', 5.568216516288637),\n",
       " ('bomb', 5.654690114292464),\n",
       " ('get', 5.667677309819275),\n",
       " ('burn', 5.792840452773281),\n",
       " ('usa', 5.833148176261374),\n",
       " ('emerg', 5.8539281447531195),\n",
       " ('flood', 5.89136567182525),\n",
       " (\"i'm\", 5.918991738100181),\n",
       " ('bodi', 5.935941296413954),\n",
       " ('attack', 5.967781902269613),\n",
       " ('via', 5.97072741249937),\n",
       " ('fatal', 6.000669769114448),\n",
       " ('crash', 6.000669769114448),\n",
       " ('suicid', 6.015984004087491),\n",
       " ('build', 6.025286396749804),\n",
       " ('evacu', 6.034676137099644)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = inverse_document_frequency_df([ df_train, df_test ])\n",
    "list(reversed(idf.most_common()))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df, freqs, use_idf=True, use_log=True, **kwargs) -> np.array:\n",
    "    features = []\n",
    "    tokens   = tokenize_df(df, **kwargs)\n",
    "    for n in range(len(tokens)):\n",
    "        bias     = 1\n",
    "        positive = 1\n",
    "        negative = 1        \n",
    "        for token in tokens[n]:\n",
    "            if use_idf:\n",
    "                positive += freqs[0].get(token, 0) * idf.get(token, 1) \n",
    "                negative += freqs[1].get(token, 0) * idf.get(token, 1)\n",
    "            else:\n",
    "                positive += freqs[0].get(token, 0) \n",
    "                negative += freqs[1].get(token, 0) \n",
    "        features.append([ positive, negative ])  \n",
    "    features = np.array(features)\n",
    "    if use_log:\n",
    "        features = np.log(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train (7613, 4)\n",
      "df_test  (3263, 3)\n",
      "Y_train  (7613,)\n",
      "X_train  (7613, 2)\n",
      "X_test   (3263, 2)\n",
      "[[6.92293033 7.38327619]\n",
      " [7.14708523 7.00546676]\n",
      " [7.29343584 8.00157928]\n",
      " [6.36825736 5.77734926]\n",
      " [5.70946644 7.56250014]]\n"
     ]
    }
   ],
   "source": [
    "Y_train = df_train['target'].to_numpy()\n",
    "X_train = extract_features(df_train, freqs)\n",
    "X_test  = extract_features(df_test,  freqs)\n",
    "print('df_train', df_train.shape)\n",
    "print('df_test ', df_test.shape)\n",
    "print('Y_train ', Y_train.shape)\n",
    "print('X_train ', X_train.shape)\n",
    "print('X_test  ', X_test.shape)\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_df(df_train, df_test, **kwargs):\n",
    "    freqs   = word_frequencies(df_train, **kwargs)\n",
    "    Y_train = df_train['target'].to_numpy()\n",
    "    X_train = extract_features(df_train, freqs, **kwargs)\n",
    "    X_test  = extract_features(df_test,  freqs, **kwargs) if df_train is not df_test else X_train\n",
    "    model      = LinearRegression().fit(X_train, Y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    prediction = np.round(prediction).astype(int)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_f1_score(splits=3, **kwargs):\n",
    "    f1 = 0.0\n",
    "    for _ in range(splits):\n",
    "        train, test = train_test_split(df_train, test_size=1/splits)      \n",
    "        prediction  = predict_df(train, test, **kwargs)\n",
    "        Y_train     = test['target'].to_numpy()\n",
    "        f1         += f1_score(Y_train, prediction, average='weighted') / splits\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_f1_score_hyperparameter_search():\n",
    "    results = Counter()\n",
    "    jobs    = []\n",
    "    for keys in [('text', 'keyword', 'location')]: \n",
    "        strip_handles = 1  \n",
    "        for stemmer, preserve_case, reduce_len, use_stopwords, use_idf, use_log in product([1,0],[1,0],[1,0],[1,0],[1,0],[1,0]):\n",
    "            def fn(keys, stemmer, preserve_case, reduce_len, strip_handles, use_stopwords, use_idf, use_log):\n",
    "                kwargs = {\n",
    "                    \"stemmer\":        stemmer,          \n",
    "                    \"preserve_case\":  preserve_case, \n",
    "                    \"reduce_len\":     reduce_len, \n",
    "                    \"use_stopwords\":  use_stopwords,    \n",
    "                    \"use_idf\":        use_idf,          \n",
    "                    \"use_log\":        use_log,          \n",
    "                }\n",
    "                label = frozendict({**kwargs})\n",
    "                f1 = get_train_f1_score(**kwargs)\n",
    "                return (label, f1)\n",
    "            jobs.append(delayed(fn)(keys, stemmer, preserve_case, reduce_len, strip_handles, use_stopwords, use_idf, use_log))\n",
    "    results = Counter(dict(Parallel(-1)(jobs)))\n",
    "    results = Counter(dict(results.most_common())) \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76096 | stemmer = 1  case = 1  len = 1  stopwords = 1  idf = 1  log = 1\n",
      "0.76036 | stemmer = 1  case = 1  len = 0  stopwords = 1  idf = 1  log = 1\n",
      "0.75823 | stemmer = 1  case = 0  len = 1  stopwords = 1  idf = 1  log = 1\n",
      "0.75786 | stemmer = 1  case = 1  len = 0  stopwords = 1  idf = 1  log = 0\n",
      "0.75376 | stemmer = 1  case = 1  len = 1  stopwords = 1  idf = 1  log = 0\n",
      "0.75364 | stemmer = 0  case = 0  len = 0  stopwords = 1  idf = 0  log = 1\n",
      "0.75110 | stemmer = 1  case = 0  len = 0  stopwords = 1  idf = 1  log = 1\n",
      "0.75097 | stemmer = 0  case = 1  len = 0  stopwords = 1  idf = 0  log = 1\n",
      "0.75070 | stemmer = 1  case = 0  len = 0  stopwords = 1  idf = 0  log = 1\n",
      "0.74989 | stemmer = 0  case = 0  len = 1  stopwords = 1  idf = 0  log = 1\n",
      "0.74751 | stemmer = 1  case = 0  len = 1  stopwords = 1  idf = 1  log = 0\n",
      "0.74703 | stemmer = 1  case = 0  len = 0  stopwords = 1  idf = 1  log = 0\n",
      "0.74650 | stemmer = 0  case = 0  len = 0  stopwords = 1  idf = 1  log = 1\n",
      "0.74470 | stemmer = 0  case = 1  len = 1  stopwords = 1  idf = 0  log = 1\n",
      "0.74278 | stemmer = 1  case = 0  len = 0  stopwords = 1  idf = 0  log = 0\n",
      "0.74124 | stemmer = 1  case = 1  len = 1  stopwords = 1  idf = 0  log = 1\n",
      "0.74106 | stemmer = 0  case = 0  len = 1  stopwords = 1  idf = 1  log = 1\n",
      "0.74097 | stemmer = 0  case = 1  len = 0  stopwords = 1  idf = 0  log = 0\n",
      "0.74034 | stemmer = 0  case = 1  len = 1  stopwords = 1  idf = 0  log = 0\n",
      "0.74020 | stemmer = 0  case = 0  len = 1  stopwords = 1  idf = 0  log = 0\n",
      "0.73976 | stemmer = 1  case = 0  len = 1  stopwords = 1  idf = 0  log = 1\n",
      "0.73799 | stemmer = 0  case = 1  len = 1  stopwords = 1  idf = 1  log = 1\n",
      "0.73670 | stemmer = 1  case = 1  len = 1  stopwords = 1  idf = 0  log = 0\n",
      "0.73640 | stemmer = 1  case = 1  len = 0  stopwords = 1  idf = 0  log = 1\n",
      "0.73487 | stemmer = 1  case = 0  len = 1  stopwords = 1  idf = 0  log = 0\n",
      "0.73273 | stemmer = 1  case = 1  len = 0  stopwords = 1  idf = 0  log = 0\n",
      "0.73128 | stemmer = 0  case = 1  len = 0  stopwords = 1  idf = 1  log = 1\n",
      "0.73087 | stemmer = 0  case = 0  len = 0  stopwords = 1  idf = 0  log = 0\n",
      "0.73046 | stemmer = 0  case = 0  len = 0  stopwords = 1  idf = 1  log = 0\n",
      "0.73042 | stemmer = 0  case = 1  len = 1  stopwords = 1  idf = 1  log = 0\n",
      "0.72397 | stemmer = 0  case = 0  len = 1  stopwords = 1  idf = 1  log = 0\n",
      "0.72247 | stemmer = 1  case = 1  len = 1  stopwords = 0  idf = 1  log = 1\n",
      "0.72227 | stemmer = 0  case = 1  len = 0  stopwords = 1  idf = 1  log = 0\n",
      "0.71933 | stemmer = 1  case = 1  len = 0  stopwords = 0  idf = 1  log = 1\n",
      "0.71909 | stemmer = 1  case = 0  len = 1  stopwords = 0  idf = 1  log = 1\n",
      "0.71732 | stemmer = 1  case = 0  len = 0  stopwords = 0  idf = 1  log = 1\n",
      "0.70860 | stemmer = 1  case = 1  len = 0  stopwords = 0  idf = 1  log = 0\n",
      "0.70793 | stemmer = 1  case = 1  len = 0  stopwords = 0  idf = 0  log = 0\n",
      "0.70614 | stemmer = 1  case = 0  len = 0  stopwords = 0  idf = 1  log = 0\n",
      "0.70485 | stemmer = 1  case = 0  len = 0  stopwords = 0  idf = 0  log = 1\n",
      "0.70139 | stemmer = 1  case = 0  len = 1  stopwords = 0  idf = 1  log = 0\n",
      "0.70060 | stemmer = 1  case = 0  len = 1  stopwords = 0  idf = 0  log = 1\n",
      "0.69949 | stemmer = 1  case = 1  len = 1  stopwords = 0  idf = 0  log = 1\n",
      "0.69759 | stemmer = 1  case = 1  len = 1  stopwords = 0  idf = 1  log = 0\n",
      "0.69737 | stemmer = 1  case = 1  len = 0  stopwords = 0  idf = 0  log = 1\n",
      "0.69636 | stemmer = 0  case = 0  len = 0  stopwords = 0  idf = 0  log = 1\n",
      "0.69556 | stemmer = 1  case = 0  len = 1  stopwords = 0  idf = 0  log = 0\n",
      "0.69474 | stemmer = 1  case = 0  len = 0  stopwords = 0  idf = 0  log = 0\n",
      "0.69422 | stemmer = 1  case = 1  len = 1  stopwords = 0  idf = 0  log = 0\n",
      "0.69402 | stemmer = 0  case = 0  len = 0  stopwords = 0  idf = 0  log = 0\n",
      "0.69230 | stemmer = 0  case = 0  len = 1  stopwords = 0  idf = 0  log = 0\n",
      "0.68741 | stemmer = 0  case = 0  len = 1  stopwords = 0  idf = 0  log = 1\n",
      "0.68737 | stemmer = 0  case = 0  len = 0  stopwords = 0  idf = 1  log = 1\n",
      "0.68621 | stemmer = 0  case = 0  len = 1  stopwords = 0  idf = 1  log = 1\n",
      "0.68204 | stemmer = 0  case = 1  len = 1  stopwords = 0  idf = 0  log = 1\n",
      "0.67940 | stemmer = 0  case = 1  len = 0  stopwords = 0  idf = 0  log = 1\n",
      "0.67786 | stemmer = 0  case = 1  len = 1  stopwords = 0  idf = 0  log = 0\n",
      "0.67731 | stemmer = 0  case = 1  len = 1  stopwords = 0  idf = 1  log = 1\n",
      "0.67364 | stemmer = 0  case = 1  len = 0  stopwords = 0  idf = 1  log = 1\n",
      "0.67069 | stemmer = 0  case = 1  len = 0  stopwords = 0  idf = 0  log = 0\n",
      "0.66881 | stemmer = 0  case = 0  len = 1  stopwords = 0  idf = 1  log = 0\n",
      "0.66508 | stemmer = 0  case = 0  len = 0  stopwords = 0  idf = 1  log = 0\n",
      "0.62854 | stemmer = 0  case = 1  len = 0  stopwords = 0  idf = 1  log = 0\n",
      "0.62754 | stemmer = 0  case = 1  len = 1  stopwords = 0  idf = 1  log = 0\n",
      "train_f1_score =  0.7570123170238943\n"
     ]
    }
   ],
   "source": [
    "results = train_f1_score_hyperparameter_search()\n",
    "for label, value in results.items():\n",
    "    print(f'{value:.5f} |', \"  \".join(f\"{k.split('_')[-1]} = {v}\" for k,v in label.items()))\n",
    "print('train_f1_score = ', get_train_f1_score())\n",
    "df_submission = pd.DataFrame({\n",
    "    \"id\":     df_test.index,\n",
    "    \"target\": predict_df(df_train, df_test)\n",
    "})\n",
    "df_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
