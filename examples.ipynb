{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015116,
     "end_time": "2021-01-05T21:06:56.654708",
     "exception": false,
     "start_time": "2021-01-05T21:06:56.639592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NLP Logistic Regression\n",
    "\n",
    "[Disaster Tweets Dataset](https://www.kaggle.com/c/nlp-getting-started)\n",
    "\n",
    "This notebook is inspired by Course 1, Week 1 of the [deeplearning.ai Natural Language Processing Specialization](https://www.deeplearning.ai/natural-language-processing-specialization/), but the code here is implemented using moden library functions rather than using hand-coded implementions.\n",
    "\n",
    "The approach here is to tokenize the text, and create word frequencies tables for each of the labels. \n",
    "A Nx2 numeric feature matrix is created for each tweet, containing the sum of positive and negative frequencies for each word tokens in the tweet.\n",
    "Linear Regression solves the problem via gradient decent, and is trained to predict labels given an extracted feature matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011662,
     "end_time": "2021-01-05T21:06:56.679182",
     "exception": false,
     "start_time": "2021-01-05T21:06:56.667520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T21:06:56.710231Z",
     "iopub.status.busy": "2021-01-05T21:06:56.709496Z",
     "iopub.status.idle": "2021-01-05T21:07:07.480428Z",
     "shell.execute_reply": "2021-01-05T21:07:07.478965Z"
    },
    "papermill": {
     "duration": 10.788855,
     "end_time": "2021-01-05T21:07:07.480554",
     "exception": false,
     "start_time": "2021-01-05T21:06:56.691699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q frozendict > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-05T21:07:07.514999Z",
     "iopub.status.busy": "2021-01-05T21:07:07.514244Z",
     "iopub.status.idle": "2021-01-05T21:07:09.202988Z",
     "shell.execute_reply": "2021-01-05T21:07:09.202281Z"
    },
    "papermill": {
     "duration": 1.709158,
     "end_time": "2021-01-05T21:07:09.203120",
     "exception": false,
     "start_time": "2021-01-05T21:07:07.493962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.12.7)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/Emulator/HocMay/BaiTieuLuan/venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy  as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk\n",
    "import pydash\n",
    "import math\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from pydash import flatten, flatten_deep\n",
    "from collections import Counter, OrderedDict\n",
    "from frozendict import frozendict\n",
    "from humanize import intcomma\n",
    "from operator import itemgetter\n",
    "from typing import *\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product, combinations\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-05T21:07:09.241878Z",
     "iopub.status.busy": "2021-01-05T21:07:09.238063Z",
     "iopub.status.idle": "2021-01-05T21:07:09.343232Z",
     "shell.execute_reply": "2021-01-05T21:07:09.343747Z"
    },
    "papermill": {
     "duration": 0.127596,
     "end_time": "2021-01-05T21:07:09.343902",
     "exception": false,
     "start_time": "2021-01-05T21:07:09.216306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10869</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10870</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows ร 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword location                                               text  \\\n",
       "id                                                                          \n",
       "1         NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "4         NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "5         NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "6         NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "7         NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "...       ...      ...                                                ...   \n",
       "10869     NaN      NaN  Two giant cranes holding a bridge collapse int...   \n",
       "10870     NaN      NaN  @aria_ahrary @TheTawniest The out of control w...   \n",
       "10871     NaN      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "10872     NaN      NaN  Police investigating after an e-bike collided ...   \n",
       "10873     NaN      NaN  The Latest: More Homes Razed by Northern Calif...   \n",
       "\n",
       "       target  \n",
       "id             \n",
       "1           1  \n",
       "4           1  \n",
       "5           1  \n",
       "6           1  \n",
       "7           1  \n",
       "...       ...  \n",
       "10869       1  \n",
       "10870       1  \n",
       "10871       1  \n",
       "10872       1  \n",
       "10873       1  \n",
       "\n",
       "[7613 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('dataset/train.csv', index_col=0)\n",
    "df_test  = pd.read_csv('dataset/test.csv', index_col=0)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013507,
     "end_time": "2021-01-05T21:07:09.373736",
     "exception": false,
     "start_time": "2021-01-05T21:07:09.360229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenization and Word Frequencies\n",
    "\n",
    "Here we tokenize the text using nltk.TweetTokenizer, apply lowercasing, tweet preprocessing, and stemming.\n",
    "\n",
    "Then compute a dictionary lookup of word counts for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T21:07:09.406574Z",
     "iopub.status.busy": "2021-01-05T21:07:09.405816Z",
     "iopub.status.idle": "2021-01-05T21:07:09.416264Z",
     "shell.execute_reply": "2021-01-05T21:07:09.417076Z"
    },
    "papermill": {
     "duration": 0.030069,
     "end_time": "2021-01-05T21:07:09.417290",
     "exception": false,
     "start_time": "2021-01-05T21:07:09.387221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T21:07:09.464880Z",
     "iopub.status.busy": "2021-01-05T21:07:09.463855Z",
     "iopub.status.idle": "2021-01-05T21:07:09.466490Z",
     "shell.execute_reply": "2021-01-05T21:07:09.467031Z"
    },
    "papermill": {
     "duration": 0.034571,
     "end_time": "2021-01-05T21:07:09.467163",
     "exception": false,
     "start_time": "2021-01-05T21:07:09.432592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_df(\n",
    "    dfs: List[pd.DataFrame], \n",
    "    keys          = ('text', 'keyword', 'location'), \n",
    "    stemmer       = True, \n",
    "    preserve_case = True, \n",
    "    reduce_len    = False, \n",
    "    strip_handles = True,\n",
    "    use_stopwords = True,\n",
    "    **kwargs,\n",
    ") -> List[List[str]]:\n",
    "    # tokenizer = nltk.TweetTokenizer(preserve_case=True,  reduce_len=False, strip_handles=False)  # defaults \n",
    "    tokenizer = nltk.TweetTokenizer(preserve_case=preserve_case, reduce_len=reduce_len, strip_handles=strip_handles) \n",
    "    porter    = nltk.PorterStemmer()\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english') + [ 'nan' ])\n",
    "\n",
    "    output    = []\n",
    "    for df in flatten([ dfs ]):\n",
    "        for index, row in df.iterrows():\n",
    "            tokens = flatten([\n",
    "                tokenizer.tokenize(str(row[key] or \"\"))\n",
    "                for key in keys    \n",
    "            ])\n",
    "            if use_stopwords:\n",
    "                tokens = [ \n",
    "                    token \n",
    "                    for token in tokens \n",
    "                    if token.lower() not in stopwords\n",
    "                    and len(token) >= 2\n",
    "                ]                \n",
    "            if stemmer:\n",
    "                tokens = [ \n",
    "                    porter.stem(token) \n",
    "                    for token in tokens \n",
    "                ]\n",
    "            output.append(tokens)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def word_frequencies(df, **kwargs) -> Dict[int, Counter]:\n",
    "    tokens = {\n",
    "        0: flatten(tokenize_df( df[df['target'] == 0], **kwargs )),\n",
    "        1: flatten(tokenize_df( df[df['target'] == 1], **kwargs )),\n",
    "    }\n",
    "    freqs = { \n",
    "        target: Counter(dict(Counter(tokens[target]).most_common())) \n",
    "        for target in [0, 1]\n",
    "    }  # sort and cast\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T21:07:09.500435Z",
     "iopub.status.busy": "2021-01-05T21:07:09.499384Z",
     "iopub.status.idle": "2021-01-05T21:07:14.721334Z",
     "shell.execute_reply": "2021-01-05T21:07:14.720781Z"
    },
    "papermill": {
     "duration": 5.239851,
     "end_time": "2021-01-05T21:07:14.721456",
     "exception": false,
     "start_time": "2021-01-05T21:07:09.481605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deed', 'reason', '#earthquak', 'may', 'allah', 'forgiv', 'us'],\n",
       " ['forest', 'fire', 'near', 'la', 'rong', 'sask', 'canada']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_df(df_train)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T21:07:14.757768Z",
     "iopub.status.busy": "2021-01-05T21:07:14.756863Z",
     "iopub.status.idle": "2021-01-05T21:07:20.076060Z",
     "shell.execute_reply": "2021-01-05T21:07:20.075266Z"
    },
    "papermill": {
     "duration": 5.340166,
     "end_time": "2021-01-05T21:07:20.076207",
     "exception": false,
     "start_time": "2021-01-05T21:07:14.736041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freqs[0] 12811 [('...', 421), ('new', 320), ('like', 309), ('get', 224), ('bodi', 216), (\"i'm\", 207), ('scream', 194), ('รป_', 171), ('burn', 159), ('obliter', 157)]\n",
      "freqs[1] 10795 [('...', 637), ('fire', 303), ('bomb', 242), ('new', 207), ('suicid', 204), ('evacu', 185), ('flood', 176), ('รป_', 171), ('derail', 170), ('kill', 160)]\n"
     ]
    }
   ],
   "source": [
    "freqs = word_frequencies(df_train)\n",
    "print('freqs[0]', len(freqs[0]), freqs[0].most_common(10))\n",
    "print('freqs[1]', len(freqs[1]), freqs[1].most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015614,
     "end_time": "2021-01-05T21:07:20.108425",
     "exception": false,
     "start_time": "2021-01-05T21:07:20.092811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Here we create a Nx2 feature matrix containing the sum of positive and negative word frequencies for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T21:07:20.147111Z",
     "iopub.status.busy": "2021-01-05T21:07:20.146478Z",
     "iopub.status.idle": "2021-01-05T21:07:27.869133Z",
     "shell.execute_reply": "2021-01-05T21:07:27.869611Z"
    },
    "papermill": {
     "duration": 7.746559,
     "end_time": "2021-01-05T21:07:27.869762",
     "exception": false,
     "start_time": "2021-01-05T21:07:20.123203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('...', 4.467633783633229),\n",
       " ('new', 5.142574999696602),\n",
       " ('fire', 5.360577151510393),\n",
       " ('like', 5.413220884995814),\n",
       " ('รป_', 5.568216516288637),\n",
       " ('bomb', 5.654690114292464),\n",
       " ('get', 5.667677309819275),\n",
       " ('burn', 5.792840452773281),\n",
       " ('usa', 5.833148176261374),\n",
       " ('emerg', 5.8539281447531195),\n",
       " ('flood', 5.89136567182525),\n",
       " (\"i'm\", 5.918991738100181),\n",
       " ('bodi', 5.935941296413954),\n",
       " ('attack', 5.967781902269613),\n",
       " ('via', 5.97072741249937),\n",
       " ('fatal', 6.000669769114448),\n",
       " ('crash', 6.000669769114448),\n",
       " ('suicid', 6.015984004087491),\n",
       " ('build', 6.025286396749804),\n",
       " ('evacu', 6.034676137099644)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverse_document_frequency( tokens: List[str] ) -> Counter:\n",
    "    tokens = flatten_deep(tokens)\n",
    "    idf = {\n",
    "        token: math.log( len(tokens) / count ) \n",
    "        for token, count in Counter(tokens).items()\n",
    "    }\n",
    "    idf = Counter(dict(Counter(idf).most_common()))  # sort and cast\n",
    "    return idf\n",
    "\n",
    "def inverse_document_frequency_df( dfs ) -> Counter:\n",
    "    tokens = flatten_deep([ tokenize_df(df) for df in flatten([ dfs ]) ])\n",
    "    return inverse_document_frequency(tokens)\n",
    "\n",
    "idf = inverse_document_frequency_df([ df_train, df_test ])\n",
    "list(reversed(idf.most_common()))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T21:07:27.904483Z",
     "iopub.status.busy": "2021-01-05T21:07:27.903569Z",
     "iopub.status.idle": "2021-01-05T21:07:35.504915Z",
     "shell.execute_reply": "2021-01-05T21:07:35.504358Z"
    },
    "papermill": {
     "duration": 7.619776,
     "end_time": "2021-01-05T21:07:35.505032",
     "exception": false,
     "start_time": "2021-01-05T21:07:27.885256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train (7613, 4)\n",
      "df_test  (3263, 3)\n",
      "Y_train  (7613,)\n",
      "X_train  (7613, 2)\n",
      "X_test   (3263, 2)\n",
      "[[6.92293033 7.38327619]\n",
      " [7.14708523 7.00546676]\n",
      " [7.29343584 8.00157928]\n",
      " [6.36825736 5.77734926]\n",
      " [5.70946644 7.56250014]]\n"
     ]
    }
   ],
   "source": [
    "def extract_features(df, freqs, use_idf=True, use_log=True, **kwargs) -> np.array:\n",
    "    features = []\n",
    "    tokens   = tokenize_df(df, **kwargs)\n",
    "    for n in range(len(tokens)):\n",
    "        bias     = 1  # bias term is implict when using sklearn\n",
    "        positive = 1\n",
    "        negative = 1        \n",
    "        for token in tokens[n]:\n",
    "            if use_idf:\n",
    "                positive += freqs[0].get(token, 0) * idf.get(token, 1) \n",
    "                negative += freqs[1].get(token, 0) * idf.get(token, 1)\n",
    "            else:\n",
    "                positive += freqs[0].get(token, 0) \n",
    "                negative += freqs[1].get(token, 0) \n",
    "        features.append([ positive, negative ])  \n",
    "\n",
    "    features = np.array(features)   # accuracy = 0.7166688559043741\n",
    "    if use_log:\n",
    "        features = np.log(features) # accuracy = 0.7136477078681204\n",
    "    return features\n",
    "\n",
    "\n",
    "Y_train = df_train['target'].to_numpy()\n",
    "X_train = extract_features(df_train, freqs)\n",
    "X_test  = extract_features(df_test,  freqs)\n",
    "\n",
    "print('df_train', df_train.shape)\n",
    "print('df_test ', df_test.shape)\n",
    "print('Y_train ', Y_train.shape)\n",
    "print('X_train ', X_train.shape)\n",
    "print('X_test  ', X_test.shape)\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01602,
     "end_time": "2021-01-05T21:07:35.537015",
     "exception": false,
     "start_time": "2021-01-05T21:07:35.520995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyperparameter Search\n",
    "\n",
    "\n",
    "The optimal settings are:\n",
    "- stemmer = True\n",
    "- preserve_case = True\n",
    "- strip_handles = Any\n",
    "- use_stopwords = True\n",
    "\n",
    "The above are exactly opposite compared to [TF-IDF Classifier](https://www.kaggle.com/jamesmcguigan/disaster-tweets-tf-idf-classifier?scriptVersionId=50898834), \n",
    "but the following settings are shared:\n",
    "\n",
    "- use_idf = True\n",
    "- use_log = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T21:07:35.588810Z",
     "iopub.status.busy": "2021-01-05T21:07:35.588127Z",
     "iopub.status.idle": "2021-01-05T21:07:35.590652Z",
     "shell.execute_reply": "2021-01-05T21:07:35.591262Z"
    },
    "papermill": {
     "duration": 0.038359,
     "end_time": "2021-01-05T21:07:35.591426",
     "exception": false,
     "start_time": "2021-01-05T21:07:35.553067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_df(df_train, df_test, **kwargs):\n",
    "    freqs   = word_frequencies(df_train, **kwargs)\n",
    "\n",
    "    Y_train = df_train['target'].to_numpy()\n",
    "    X_train = extract_features(df_train, freqs, **kwargs)\n",
    "    X_test  = extract_features(df_test,  freqs, **kwargs) \\\n",
    "              if df_train is not df_test else X_train\n",
    "\n",
    "    model      = LinearRegression().fit(X_train, Y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    prediction = np.round(prediction).astype(np.int)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def get_train_accuracy(splits=3, **kwargs):\n",
    "    \"\"\" K-Fold Split Accuracy \"\"\"\n",
    "    accuracy = 0.0\n",
    "    for _ in range(splits):\n",
    "        train, test = train_test_split(df_train, test_size=1/splits)      \n",
    "        prediction  = predict_df(train, test, **kwargs)\n",
    "        Y_train     = test['target'].to_numpy()\n",
    "        accuracy   += np.sum( Y_train == prediction ) / len(Y_train) / splits    \n",
    "    return accuracy\n",
    "    \n",
    "    \n",
    "def train_accuracy_hyperparameter_search():\n",
    "    results = Counter()\n",
    "    jobs    = []\n",
    "    \n",
    "    # NOTE: reducing input fields has no effect on accuracy\n",
    "    for keys in [ ('text', 'keyword', 'location'), ]: # ('text', 'keyword'), ('text',) ]:\n",
    "        strip_handles = 1  # no effect on accuracy \n",
    "        # use_log       = 1  # no effect on accuracy\n",
    "        for stemmer, preserve_case, reduce_len, use_stopwords, use_idf, use_log in product([1,0],[1,0],[1,0],[1,0],[1,0],[1,0]):\n",
    "            def fn(keys, stemmer, preserve_case, reduce_len, strip_handles, use_stopwords, use_idf, use_log):\n",
    "                kwargs = {\n",
    "                    \"stemmer\":        stemmer,          # stemmer = True is always better\n",
    "                    \"preserve_case\":  preserve_case, \n",
    "                    \"reduce_len\":     reduce_len, \n",
    "                    # \"strip_handles\": strip_handles,   # no effect on accuracy\n",
    "                    \"use_stopwords\":  use_stopwords,    # use_stopwords = True is always better\n",
    "                    \"use_idf\":        use_idf,          # use_idf = True is always better\n",
    "                    \"use_log\":        use_log,          # use_log = True is always better\n",
    "                }\n",
    "                label = frozendict({\n",
    "                    **kwargs,\n",
    "                    # \"keys\": keys,                     # no effect on accuracy\n",
    "                })\n",
    "                accuracy = get_train_accuracy(**kwargs)\n",
    "                return (label, accuracy)\n",
    "            \n",
    "            # hyperparameter search is slow, so multiprocess it\n",
    "            jobs.append( delayed(fn)(keys, stemmer, preserve_case, reduce_len, strip_handles, use_stopwords, use_idf, use_log) )\n",
    "            \n",
    "    results = Counter(dict( Parallel(-1)(jobs) ))\n",
    "    results = Counter(dict(results.most_common()))  # sort and cast\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T21:07:35.631191Z",
     "iopub.status.busy": "2021-01-05T21:07:35.627447Z",
     "iopub.status.idle": "2021-01-05T21:17:01.462389Z",
     "shell.execute_reply": "2021-01-05T21:17:01.462996Z"
    },
    "papermill": {
     "duration": 565.854803,
     "end_time": "2021-01-05T21:17:01.463163",
     "exception": false,
     "start_time": "2021-01-05T21:07:35.608360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"e:\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"e:\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"e:\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\duytung\\AppData\\Local\\Temp\\ipykernel_10624\\2970203127.py\", line 49, in fn\n  File \"C:\\Users\\duytung\\AppData\\Local\\Temp\\ipykernel_10624\\2970203127.py\", line 20, in get_train_accuracy\n  File \"C:\\Users\\duytung\\AppData\\Local\\Temp\\ipykernel_10624\\2970203127.py\", line 11, in predict_df\n  File \"e:\\Python312\\Lib\\site-packages\\numpy\\__init__.py\", line 338, in __getattr__\n    raise AttributeError(__former_attrs__[attr])\nAttributeError: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'inf'?\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[27], line 55\u001b[0m, in \u001b[0;36mtrain_accuracy_hyperparameter_search\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;66;03m# hyperparameter search is slow, so multiprocess it\u001b[39;00m\n\u001b[0;32m     53\u001b[0m         jobs\u001b[38;5;241m.\u001b[39mappend( delayed(fn)(keys, stemmer, preserve_case, reduce_len, strip_handles, use_stopwords, use_idf, use_log) )\n\u001b[1;32m---> 55\u001b[0m results \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mdict\u001b[39m( \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m ))\n\u001b[0;32m     56\u001b[0m results \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mdict\u001b[39m(results\u001b[38;5;241m.\u001b[39mmost_common()))  \u001b[38;5;66;03m# sort and cast\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32me:\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Python312\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Python312\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = train_accuracy_hyperparameter_search()\n",
    "for label, value in results.items():\n",
    "    print(f'{value:.5f} |', \"  \".join(f\"{k.split('_')[-1]} = {v}\" for k,v in label.items() ))  # pretty printdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017274,
     "end_time": "2021-01-05T21:17:01.498130",
     "exception": false,
     "start_time": "2021-01-05T21:17:01.480856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Verify train accuracy given default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T21:17:01.537627Z",
     "iopub.status.busy": "2021-01-05T21:17:01.537008Z",
     "iopub.status.idle": "2021-01-05T21:17:28.087497Z",
     "shell.execute_reply": "2021-01-05T21:17:28.088316Z"
    },
    "papermill": {
     "duration": 26.573481,
     "end_time": "2021-01-05T21:17:28.088559",
     "exception": false,
     "start_time": "2021-01-05T21:17:01.515078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_accuracy = \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mget_train_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[14], line 20\u001b[0m, in \u001b[0;36mget_train_accuracy\u001b[1;34m(splits, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(splits):\n\u001b[0;32m     19\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m train_test_split(df_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39msplits)      \n\u001b[1;32m---> 20\u001b[0m     prediction  \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     Y_train     \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m     22\u001b[0m     accuracy   \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum( Y_train \u001b[38;5;241m==\u001b[39m prediction ) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(Y_train) \u001b[38;5;241m/\u001b[39m splits    \n",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m, in \u001b[0;36mpredict_df\u001b[1;34m(df_train, df_test, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m model      \u001b[38;5;241m=\u001b[39m LinearRegression()\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[0;32m     10\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m---> 11\u001b[0m prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(prediction)\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prediction\n",
      "File \u001b[1;32me:\\Python312\\Lib\\site-packages\\numpy\\__init__.py:338\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    333\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "print('train_accuracy = ', get_train_accuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024587,
     "end_time": "2021-01-05T21:17:28.141790",
     "exception": false,
     "start_time": "2021-01-05T21:17:28.117203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission\n",
    "\n",
    "Without additional feature engineering, LinearRegression scores worst than my [TF-IDF Classifier](https://www.kaggle.com/jamesmcguigan/disaster-tweets-tf-idf-classifier?scriptVersionId=50898834)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T21:17:28.198320Z",
     "iopub.status.busy": "2021-01-05T21:17:28.197712Z",
     "iopub.status.idle": "2021-01-05T21:17:42.158192Z",
     "shell.execute_reply": "2021-01-05T21:17:42.158716Z"
    },
    "papermill": {
     "duration": 13.990587,
     "end_time": "2021-01-05T21:17:42.158869",
     "exception": false,
     "start_time": "2021-01-05T21:17:28.168282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,target\r\n",
      "0,1\r\n",
      "2,0\r\n",
      "3,1\r\n",
      "9,0\r\n",
      "11,1\r\n",
      "12,1\r\n",
      "21,0\r\n",
      "22,0\r\n",
      "27,0\r\n"
     ]
    }
   ],
   "source": [
    "df_submission = pd.DataFrame({\n",
    "    \"id\":     df_test.index,\n",
    "    \"target\": predict_df(df_train, df_test)\n",
    "})\n",
    "df_submission.to_csv('submission.csv', index=False)\n",
    "! head submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01832,
     "end_time": "2021-01-05T21:17:42.196056",
     "exception": false,
     "start_time": "2021-01-05T21:17:42.177736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Further Reading\n",
    "\n",
    "This notebook is part of a series exploring Natural Language Processing\n",
    "- 0.74164 - [NLP Logistic Regression](https://www.kaggle.com/jamesmcguigan/disaster-tweets-logistic-regression/)\n",
    "- 0.77536 - [NLP TF-IDF Classifier](https://www.kaggle.com/jamesmcguigan/disaster-tweets-tf-idf-classifier)\n",
    "- 0.79742 - [NLP Naive Bayes](https://www.kaggle.com/jamesmcguigan/nlp-naive-bayes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "duration": 650.966115,
   "end_time": "2021-01-05T21:17:43.235071",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-05T21:06:52.268956",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
